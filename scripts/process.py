#!/usr/bin/env python3
import os
import sys
import zipfile
import shutil
import re
from datetime import datetime
from bs4 import BeautifulSoup, NavigableString

def main():
    print("=" * 50)
    print("Starting EPUB processing...")
    print("=" * 50)
    
    # æ¸…ç†
    if os.path.exists('output'):
        shutil.rmtree('output')
    if os.path.exists('temp_epub'):
        shutil.rmtree('temp_epub')
    
    os.makedirs('output/articles', exist_ok=True)
    os.makedirs('output/images', exist_ok=True)
    
    # è§£å‹
    print("Extracting EPUB...")
    with zipfile.ZipFile('input/economist.epub', 'r') as z:
        z.extractall('temp_epub')
    
    # å¤åˆ¶å›¾ç‰‡
    copy_images('temp_epub', 'output/images')
    
    # ç¬¬ä¸€æ­¥ï¼šè¯»å–ç›®å½•é¡µï¼Œè·å– section é¡ºåº
    sections_order = get_sections_order('temp_epub')
    print(f"\nFound sections in order: {sections_order}")
    
    # ç¬¬äºŒæ­¥ï¼šæŒ‰é¡ºåºå¤„ç†æ¯ä¸ªæ–‡ä»¶
    all_articles = []
    processed_files = set()
    
    # è·å– spine é¡ºåºï¼ˆEPUB é˜…è¯»é¡ºåºï¼‰
    spine_files = get_spine_order('temp_epub')
    print(f"Spine files: {len(spine_files)}")
    
    for filepath in spine_files:
        if filepath in processed_files:
            continue
        
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ Economist å†…å®¹
            if 'The Economist' not in content and 'economist.com' not in content:
                continue
            
            articles = parse_html_file(content, filepath, sections_order)
            all_articles.extend(articles)
            processed_files.add(filepath)
            
        except Exception as e:
            print(f"Error in {os.path.basename(filepath)}: {e}")
    
    print(f"\nTotal articles: {len(all_articles)}")
    
    if not all_articles:
        print("ERROR: No articles found!")
        sys.exit(1)
    
    # å»é‡ï¼ˆæŒ‰ slugï¼‰
    seen = set()
    unique_articles = []
    for art in all_articles:
        if art['slug'] not in seen:
            seen.add(art['slug'])
            unique_articles.append(art)
    
    print(f"Unique articles: {len(unique_articles)}")
    
    # ç”Ÿæˆç½‘ç«™
    generate_index(unique_articles, sections_order)
    generate_rss(unique_articles)
    
    shutil.rmtree('temp_epub')
    print(f"\nSuccess! Generated {len(unique_articles)} articles")

def get_sections_order(epub_root):
    """ä»ç›®å½•é¡µè·å– section é¡ºåº"""
    sections = []
    
    # æ‰¾ç›®å½•æ–‡ä»¶
    toc_files = ['nav.xhtml', 'toc.ncx', 'toc.html', 'book_toc.html']
    
    for toc_file in toc_files:
        toc_path = find_file(epub_root, toc_file)
        if toc_path:
            try:
                with open(toc_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                soup = BeautifulSoup(content, 'html.parser')
                
                # æ‰¾æ‰€æœ‰é“¾æ¥æ–‡æœ¬
                for link in soup.find_all('a'):
                    text = link.get_text(strip=True)
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ section åç§°
                    if text and len(text) < 100 and not text.startswith('http'):
                        # æ’é™¤å¹¿å‘Šå’Œæ— å…³å†…å®¹
                        if any(keyword in text.lower() for keyword in 
                               ['the world this week', 'leaders', 'letters', 'by invitation', 
                                'briefing', 'united states', 'the americas', 'asia', 'china',
                                'middle east', 'africa', 'europe', 'britain', 'international',
                                'business', 'finance', 'science', 'technology', 'culture',
                                'economic', 'financial indicators', 'obituary']):
                            if text not in sections:
                                sections.append(text)
                
                if sections:
                    break
                    
            except Exception as e:
                print(f"Error reading toc: {e}")
    
    return sections

def get_spine_order(epub_root):
    """è·å– EPUB çš„é˜…è¯»é¡ºåº"""
    files = []
    
    # æ‰¾ content.opf
    opf_path = find_file(epub_root, '.opf')
    if opf_path:
        try:
            with open(opf_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # è§£æ spine
            import xml.etree.ElementTree as ET
            root = ET.fromstring(content)
            
            # æ‰¾ manifest
            manifest = {}
            for item in root.findall('.//{http://www.idpf.org/2007/opf}item'):
                item_id = item.get('id')
                item_href = item.get('href')
                if item_id and item_href:
                    manifest[item_id] = os.path.join(os.path.dirname(opf_path), item_href)
            
            # æ‰¾ spine
            for itemref in root.findall('.//{http://www.idpf.org/2007/opf}itemref'):
                item_id = itemref.get('idref')
                if item_id in manifest:
                    files.append(manifest[item_id])
            
        except Exception as e:
            print(f"Error parsing spine: {e}")
    
    # å¦‚æœ spine è§£æå¤±è´¥ï¼ŒæŒ‰æ–‡ä»¶åæ’åº
    if not files:
        for root, dirs, filenames in os.walk(epub_root):
            for f in filenames:
                if f.endswith(('.html', '.xhtml', '.htm')):
                    files.append(os.path.join(root, f))
        files.sort()
    
    return files

def parse_html_file(html_content, filepath, sections_order):
    """è§£æ HTML æ–‡ä»¶"""
    articles = []
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # ç§»é™¤ script å’Œ style
    for tag in soup(['script', 'style']):
        tag.decompose()
    
    # è·å– body
    body = soup.find('body')
    if not body:
        return articles
    
    # ç­–ç•¥1ï¼šæ‰¾ section æ ‡è®°ï¼ˆå¦‚ "Leaders | Greenback danger"ï¼‰
    # ç­–ç•¥2ï¼šæŒ‰ h1/h2 ç»“æ„è§£æ
    
    full_text = body.get_text('\n', strip=True)
    
    # å°è¯•è¯†åˆ« section å’Œæ ‡é¢˜
    # æ¨¡å¼ï¼šSection Name | Subtitleï¼ˆå¯èƒ½åœ¨ h1, h2, æˆ–æ™®é€šæ–‡æœ¬ï¼‰
    
    # å…ˆæ‰¾æ˜ç¡®çš„ section æ ‡è®°
    section_pattern = r'(The world this week|Leaders|Letters|By Invitation|Briefing|United States|The Americas|Asia|China|Middle East & Africa|Europe|Britain|International|Business|Finance & economics|Science & technology|Culture|Economic & financial indicators|Obituary)\s*\|\s*([^\n]+)'
    
    matches = list(re.finditer(section_pattern, full_text, re.IGNORECASE))
    
    if matches:
        # æŒ‰ section åˆ†å‰²æ–‡ç« 
        for i, match in enumerate(matches):
            section_name = match.group(1).strip()
            subtitle = match.group(2).strip()
            
            # æå–è¿™æ®µå†…å®¹
            start = match.end()
            end = matches[i+1].start() if i+1 < len(matches) else len(full_text)
            section_text = full_text[start:end]
            
            # è§£æè¿™ç¯‡æ–‡ç« 
            article = parse_article_text(section_text, section_name, subtitle)
            if article:
                articles.append(article)
                print(f"  âœ“ [{section_name}] {article['title'][:50]}...")
    
    else:
        # æ²¡æœ‰ section æ ‡è®°ï¼Œå°è¯•ä» HTML ç»“æ„è§£æ
        # æ‰¾ h1 ä½œä¸ºä¸»æ ‡é¢˜
        h1 = body.find('h1')
        if h1:
            title = h1.get_text(strip=True)
            
            # æ‰¾æ—¥æœŸï¼ˆå¯èƒ½åœ¨ h2, h3, æˆ–åé¢çš„æ–‡æœ¬ï¼‰
            date = ""
            for tag in h1.find_next_siblings(['h2', 'h3', 'p']):
                text = tag.get_text(strip=True)
                if re.search(r'(January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2}', text):
                    date = text
                    break
            
            # è·å–å†…å®¹ï¼ˆä¿ç•™ HTML ç»“æ„ï¼‰
            content_html = get_content_html(h1)
            
            if len(content_html) > 200:
                article = create_article(title, date, "", content_html)
                if article:
                    articles.append(article)
                    print(f"  âœ“ [No section] {title[:50]}...")
    
    return articles

def parse_article_text(text, section_name, subtitle):
    """ä»æ–‡æœ¬è§£æå•ç¯‡æ–‡ç« """
    lines = [l.strip() for l in text.split('\n') if l.strip()]
    
    if not lines:
        return None
    
    # ç¬¬ä¸€è¡Œé€šå¸¸æ˜¯ä¸»æ ‡é¢˜ï¼ˆå¤§å†™å¼€å¤´ï¼Œè¾ƒé•¿ï¼‰
    # ä½†éœ€è¦æ’é™¤æ—¥æœŸ
    title = ""
    date = ""
    content_start = 0
    
    for i, line in enumerate(lines):
        # è·³è¿‡ subtitle é‡å¤
        if line == subtitle or subtitle in line:
            continue
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ—¥æœŸ
        if re.search(r'(January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2}(st|nd|rd|th)?\s+202[56]', line):
            date = line
            content_start = i + 1
            continue
        
        # æ‰¾æ ‡é¢˜ï¼ˆä¸æ˜¯æ—¥æœŸï¼Œé•¿åº¦é€‚ä¸­ï¼Œå¤§å†™å¼€å¤´ï¼‰
        if not title and len(line) > 10 and len(line) < 200 and line[0].isupper():
            # æ£€æŸ¥ä¸æ˜¯çº¯æ—¥æœŸ
            if not re.match(r'^\d{1,2}\s+(January|February|March|April|May|June|July|August|September|October|November|December)', line):
                title = line
                content_start = i + 1
                continue
    
    # å¦‚æœæ²¡æ‰¾åˆ°æ ‡é¢˜ï¼Œç”¨ subtitle
    if not title:
        title = subtitle
    
    # å¦‚æœè¿˜æ˜¯æ²¡æ ‡é¢˜ï¼Œè·³è¿‡
    if not title or title == date:
        return None
    
    # æå–å†…å®¹
    content_lines = lines[content_start:]
    
    # æ¸…ç†å†…å®¹ï¼ˆä¿ç•™æ®µè½ï¼‰
    content = '\n\n'.join(content_lines)
    
    # ç§»é™¤ä¸‹è½½ä¿¡æ¯
    content = re.sub(r'This article was downloaded by zlibrary from https?://\S+', '', content)
    
    if len(content) < 100:
        return None
    
    return create_article(title, date, section_name, content)

def get_content_html(start_tag):
    """è·å–ä» start_tag ä¹‹åçš„å†…å®¹ HTML"""
    content = []
    
    for sibling in start_tag.find_next_siblings():
        # å¦‚æœé‡åˆ°æ–°çš„ h1ï¼Œåœæ­¢
        if sibling.name == 'h1':
            break
        
        # ä¿ç•™æ ‡ç­¾
        content.append(str(sibling))
    
    return '\n'.join(content)

def create_article(title, date, section, content):
    """åˆ›å»ºæ–‡ç« æ–‡ä»¶"""
    
    # æ¸…ç†æ ‡é¢˜
    title = re.sub(r'\s+', ' ', title).strip()
    if len(title) > 150:
        title = title[:147] + "..."
    
    # å¦‚æœæ ‡é¢˜æ˜¯æ—¥æœŸï¼Œå°è¯•ç”¨ section æˆ–å…¶ä»–ä¿¡æ¯
    if re.match(r'^\d{1,2}\s+(January|February|March|April|May|June|July|August|September|October|November|December)', title):
        if section:
            title = f"{section} - {title}"
        else:
            title = "Article - " + title
    
    # ç”Ÿæˆ slug
    slug = re.sub(r'[^\w\s-]', '', title).strip().replace(' ', '-').lower()[:50]
    slug = re.sub(r'-+', '-', slug)
    
    # ç¡®ä¿å”¯ä¸€
    base_slug = slug
    counter = 1
    while os.path.exists(f'output/articles/{slug}.html'):
        slug = f"{base_slug}-{counter}"
        counter += 1
    
    # å¤„ç†å†…å®¹
    # å¦‚æœæ˜¯çº¯æ–‡æœ¬ï¼Œè½¬æ¢ä¸ºæ®µè½
    if not content.strip().startswith('<'):
        paragraphs = content.split('\n\n')
        paragraphs = [f'<p>{p.strip()}</p>' for p in paragraphs if p.strip()]
        content = '\n'.join(paragraphs)
    
    # ä¿®å¤å›¾ç‰‡è·¯å¾„
    content = re.sub(r'src=["\']static_images/', 'src="/images/', content)
    content = re.sub(r'src=["\']../static_images/', 'src="/images/', content)
    content = re.sub(r'src=["\']../../static_images/', 'src="/images/', content)
    
    art_path = f'articles/{slug}.html'
    
    html = f'''<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>{title} | The Economist</title>
    <style>
        body {{
            max-width: 720px;
            margin: 0 auto;
            padding: 40px 20px;
            font-family: Georgia, "Times New Roman", serif;
            font-size: 18px;
            line-height: 1.6;
            color: #222;
        }}
        .section {{
            color: #e3120b;
            font-size: 12px;
            text-transform: uppercase;
            letter-spacing: 1px;
            margin-bottom: 8px;
        }}
        h1 {{
            font-size: 32px;
            margin: 0 0 10px 0;
            line-height: 1.2;
            font-weight: normal;
        }}
        .date {{
            color: #666;
            font-size: 14px;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }}
        p {{
            margin: 0 0 1em 0;
            text-align: justify;
        }}
        img {{
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
        }}
        a {{
            color: #e3120b;
            text-decoration: none;
        }}
        a:hover {{
            text-decoration: underline;
        }}
    </style>
</head>
<body>
    {f'<div class="section">{section}</div>' if section else ''}
    <h1>{title}</h1>
    {f'<div class="date">{date}</div>' if date else ''}
    {content}
</body>
</html>'''
    
    with open(f'output/{art_path}', 'w', encoding='utf-8') as f:
        f.write(html)
    
    return {
        'title': title,
        'slug': slug,
        'path': art_path,
        'date': datetime.now().isoformat(),
        'section': section
    }

def generate_index(articles, sections_order):
    """ç”Ÿæˆç´¢å¼•é¡µï¼Œä¿æŒ section é¡ºåº"""
    repo = os.environ.get('GITHUB_REPOSITORY', 'user/repo')
    username, repo_name = repo.split('/')
    base_url = f"https://{username}.github.io/{repo_name}"
    
    # æŒ‰ section åˆ†ç»„ï¼Œä¿æŒé¡ºåº
    by_section = {}
    section_positions = {}
    
    for art in articles:
        sec = art.get('section', 'Other')
        if sec not in by_section:
            by_section[sec] = []
        by_section[sec].append(art)
    
    # ç¡®å®š section é¡ºåº
    ordered_sections = []
    
    # å…ˆæŒ‰ sections_order ä¸­çš„é¡ºåº
    for sec in sections_order:
        # æ¨¡ç³ŠåŒ¹é…
        for key in by_section.keys():
            if sec.lower() in key.lower() or key.lower() in sec.lower():
                if key not in ordered_sections:
                    ordered_sections.append(key)
                    section_positions[key] = len(ordered_sections)
    
    # æ·»åŠ å‰©ä½™çš„ section
    for key in by_section.keys():
        if key not in ordered_sections:
            ordered_sections.append(key)
            section_positions[key] = 999
    
    # å¯¹æ¯ä¸ª section å†…çš„æ–‡ç« ï¼Œä¿æŒåŸå§‹é¡ºåºï¼ˆæŒ‰æ–‡ä»¶å¤„ç†é¡ºåºï¼‰
    # articles åˆ—è¡¨å·²ç»æ˜¯æŒ‰é¡ºåºçš„ï¼Œæ‰€ä»¥ by_section ä¸­çš„é¡ºåºä¹Ÿæ˜¯å¯¹çš„
    
    html = f'''<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>The Economist Weekly</title>
    <style>
        body {{
            max-width: 800px;
            margin: 40px auto;
            padding: 0 20px;
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            background: #f5f5f5;
        }}
        .container {{
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        h1 {{
            color: #e3120b;
            margin-bottom: 8px;
        }}
        .subtitle {{
            color: #666;
            margin-bottom: 30px;
        }}
        .section-title {{
            color: #e3120b;
            font-size: 14px;
            text-transform: uppercase;
            letter-spacing: 1px;
            margin: 30px 0 15px 0;
            padding-bottom: 5px;
            border-bottom: 2px solid #e3120b;
        }}
        .article {{
            border-bottom: 1px solid #eee;
            padding: 12px 0;
        }}
        .article:hover {{
            background: #fafafa;
            margin: 0 -40px;
            padding-left: 40px;
            padding-right: 40px;
        }}
        .article a {{
            color: #222;
            text-decoration: none;
            font-size: 16px;
            display: block;
        }}
        .article a:hover {{
            color: #e3120b;
        }}
        .rss {{
            display: inline-block;
            margin-top: 30px;
            padding: 12px 24px;
            background: #e3120b;
            color: white;
            text-decoration: none;
            border-radius: 6px;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>The Economist</h1>
        <div class="subtitle">{len(articles)} articles â€¢ Updated {datetime.now().strftime("%Y-%m-%d")}</div>
'''
    
    for sec in ordered_sections:
        if sec in by_section and by_section[sec]:
            html += f'<div class="section-title">{sec}</div>\n'
            for art in by_section[sec]:
                html += f'<div class="article"><a href="{art["path"]}">{art["title"]}</a></div>\n'
    
    html += f'''
        <a href="feed.xml" class="rss">ğŸ“¡ Subscribe via RSS</a>
    </div>
</body>
</html>'''
    
    with open('output/index.html', 'w', encoding='utf-8') as f:
        f.write(html)

def generate_rss(articles):
    """ç”Ÿæˆ RSS"""
    repo = os.environ.get('GITHUB_REPOSITORY', 'user/repo')
    username, repo_name = repo.split('/')
    base_url = f"https://{username}.github.io/{repo_name}"
    
    items = []
    for art in articles[:30]:
        try:
            with open(f'output/{art["path"]}', 'r', encoding='utf-8') as f:
                content = f.read()
            
            match = re.search(r'<body>(.*?)</body>', content, re.DOTALL)
            body = match.group(1) if match else content
            
            items.append(f'''
    <item>
      <title><![CDATA[{art["title"]}]]></title>
      <link>{base_url}/{art["path"]}</link>
      <guid>{base_url}/{art["path"]}</guid>
      <description><![CDATA[{art.get("section", "")} - {art["title"]}]]></description>
      <content:encoded xmlns:content="http://purl.org/rss/1.0/modules/content/">
        <![CDATA[{body}]]>
      </content:encoded>
    </item>''')
        except Exception as e:
            print(f"Warning: RSS error for {art['title']}: {e}")
    
    rss = f'''<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>The Economist Weekly</title>
    <link>{base_url}/</link>
    <description>Full-text articles from The Economist</description>
    <language>en</language>
    <lastBuildDate>{datetime.now().strftime("%a, %d %b %Y %H:%M:%S +0000")}</lastBuildDate>
    {''.join(items)}
  </channel>
</rss>'''
    
    with open('output/feed.xml', 'w', encoding='utf-8') as f:
        f.write(rss)
    
    print(f"RSS: {base_url}/feed.xml")

def find_file(root, filename):
    """æŸ¥æ‰¾æ–‡ä»¶"""
    for r, d, files in os.walk(root):
        for f in files:
            if f == filename or f.endswith(filename):
                return os.path.join(r, f)
    return None

def copy_images(source_dir, output_dir):
    """å¤åˆ¶å›¾ç‰‡"""
    for r, dirs, files in os.walk(source_dir):
        if 'images' in r.lower():
            for f in files:
                if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.svg', '.webp')):
                    src = os.path.join(r, f)
                    dst = os.path.join(output_dir, f)
                    shutil.copy2(src, dst)

if __name__ == '__main__':
    main()
